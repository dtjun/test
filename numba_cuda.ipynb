{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用名词\n",
    "\n",
    "• host: the CPU\n",
    "\n",
    "• device: the GPU\n",
    "\n",
    "• host memory: the system main memory\n",
    "\n",
    "• device memory: onboard memory on a GPU card\n",
    "\n",
    "• kernels: 被主机执行，在设备中计算\n",
    "\n",
    "• device function: 设备函数，只能被kernel函数调用或被其他设备函数调用\n",
    "\n",
    "• blockspergrid：线程块数\n",
    "\n",
    "• threadsperblock：线程数，每个线程块的线程有共享内存，基本互相不会影响速度，所以要设置的尽可能大，只要不会超内存就行，一般设为128、256、512、1024等\n",
    "    ** 这两个参数都可以设置为单个整数或维度为1、2、3的tupple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算当前线程的位置：\n",
    "    cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    当前线程数+线程块数*线程块的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 直接返回绝对位置：\n",
    "    numba.cuda.grid(ndim)： 返回当前位置在所有线程块的绝对位置，ndim为xyz的个数\n",
    "    numba.cuda.gridsize(ndim)：返回其维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 内存控制：\n",
    "    numba.cuda.device_array(shape, dtype=np.float_, strides=None, order='C', stream=0)：声明一个空向量\n",
    "    numba.cuda.device_array_like(ary, stream=0)：同上\n",
    "    numba.cuda.to_device(obj, stream=0, copy=True, to=None)：返回一个copy到设备的变量\n",
    "    numba.copy_to_host()：返回一个copy到主机的变量\n",
    "    numba.cuda.as_cuda_array(obj, sync=True):不复制数据，创造一个DeviceNDArray对象\n",
    "    numba.cuda.is_cuda_array(obj)：检测该对象是否被定义为__cuda_array_interface__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设备向量\n",
    "    类：class numba.cuda.cudadrv.devicearray.DeviceNDArray(shape, strides, dtype, stream=0, gpu_data=None)\n",
    "    方法：copy_to_host(ary=None, stream=0)\n",
    "        is_c_contiguous()：判断向量是否行存储在内存中的地址是连续的\n",
    "        is_f_contiguous()：列\n",
    "        ravel(order='C', stream=0)：展平处理\n",
    "        reshape(*newshape, **kws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 固定内存\n",
    "    numba.cuda.pinned(*arylist):A context manager for temporary pinning a sequence of host ndarrays.没理解\n",
    "    numba.cuda.pinned_array(shape, dtype=np.float_, strides=None, order='C')：初始化一个空的固定内存向量\n",
    "    numba.cuda.pinned_array_like(ary)：同上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 映射内存\n",
    "    numba.cuda.mapped(*arylist, **kws)：同上没理解\n",
    "    numba.cuda.mapped_array(shape, dtype=np.float_, strides=None, order='C', stream=0, portable=False,wc=False)：初始化\n",
    "    numba.cuda.mapped_array_like(ary, stream=0, portable=False, wc=False)：同上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 共享内存和线程同步\n",
    "   numba.cuda.shared.array(shape, type)：初始化，shape可以为整数或tuple ，在一个线程块中共享  \n",
    "   <font color=#FF0000 > numba.cuda.syncthreads():同步一个线程块内的所有线程 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本地内存\n",
    "    numba.cuda.local.array(shape, type)：初始化，属于单个线程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常量内存\n",
    "    numba.cuda.const.array_like(arr)：只可读，不可写的内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 释放内存\n",
    "    numba.cuda.defer_cleanup()：\n",
    "    案例\n",
    "    with defer_cleanup():\n",
    "        # all cleanup is deferred in here\n",
    "        do_speed_critical_code()\n",
    "        # cleanup can occur here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 核函数\n",
    "    没有返回值，输入输出都要在参数列表内"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设备函数\n",
    "    可以有返回值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不支持的python结构：\n",
    "    • Exception handling (try .. except, try .. finally)\n",
    "    • Context management (the with statement)\n",
    "    • Comprehensions (either list, dict, set or generator comprehensions)\n",
    "    • Generator (any yield statements)\n",
    "### 支持的python结构：\n",
    "    • raise\n",
    "    • assert\n",
    "    • Printing of strings, integers, and floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 支持的内置函数以及包\n",
    "    • abs()\n",
    "    • bool\n",
    "    • complex\n",
    "    • enumerate()\n",
    "    • float\n",
    "    • int: only the one-argument form\n",
    "    • len()\n",
    "    • min(): only the multiple-argument form\n",
    "    • max(): only the multiple-argument form\n",
    "    • pow()\n",
    "    • range\n",
    "    • round()\n",
    "    • zip()\n",
    "####    cmath module\n",
    "####    math\n",
    "####    operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 支持的原子钟计算\n",
    "    class numba.cuda.atomic：再接一个方法即可\n",
    "    案例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997580056812965 == 0.9997580056812965\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "@cuda.jit\n",
    "def max_example_3d(result, values):\n",
    "    \"\"\"\n",
    "    Find the maximum value in values and store in result[0].\n",
    "    Both result and values are 3d arrays.\n",
    "    \"\"\"\n",
    "    i, j, k = cuda.grid(3)\n",
    "    # Atomically store to result[0,1,2] from values[i, j, k]\n",
    "    cuda.atomic.max(result, (0, 1, 2), values[i, j, k])\n",
    "arr = np.random.rand(1000).reshape(10,10,10)\n",
    "result = np.zeros((3, 3, 3), dtype=np.float64)\n",
    "max_example_3d[(2, 2, 2), (5, 5, 5)](result, arr)\n",
    "print(result[0, 1, 2], '==', np.max(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机数字\n",
    "    numba.cuda.random.create_xoroshiro128p_states(n, seed, subsequence_start=0, stream=0)：返回一个生成长度为n的随机数字生成器\n",
    "    numba.cuda.random.init_xoroshiro128p_states(states, seed, subsequence_start=0, stream=0)\n",
    "    以下都是一些生成随机数字的方法，参数为生成器和id（线程id），返回单个随机数\n",
    "    numba.cuda.random.xoroshiro128p_normal_float32(states, index)\n",
    "    numba.cuda.random.xoroshiro128p_normal_float64(states, index)\n",
    "    numba.cuda.random.xoroshiro128p_uniform_float32(states, index)\n",
    "    numba.cuda.random.xoroshiro128p_uniform_float64(states, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一维度随机数生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi: 3.1416733\n",
      "0.2082970142364502\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "from numba import cuda\n",
    "from numba.cuda.random import create_xoroshiro128p_states, xoroshiro128p_uniform_float32\n",
    "import numpy as np\n",
    "@cuda.jit\n",
    "def compute_pi(rng_states, iterations, out):\n",
    "    \"\"\"Find the maximum value in values and store in result[0]\"\"\"\n",
    "    thread_id = cuda.grid(1)\n",
    "    # Compute pi by drawing random (x, y) points and finding what\n",
    "    # fraction lie inside a unit circle\n",
    "    inside = 0\n",
    "    for i in range(iterations):\n",
    "        x = xoroshiro128p_uniform_float32(rng_states, thread_id)\n",
    "        y = xoroshiro128p_uniform_float32(rng_states, thread_id)\n",
    "        if x**2 + y**2 <= 1.0:\n",
    "            inside += 1\n",
    "    out[thread_id] = 4.0 * inside / iterations\n",
    "import time\n",
    "start = time.time()\n",
    "threads_per_block = 64\n",
    "blocks = 24\n",
    "rng_states = create_xoroshiro128p_states(threads_per_block * blocks, seed=1)\n",
    "out = np.zeros(threads_per_block * blocks, dtype=np.float32)\n",
    "compute_pi[blocks, threads_per_block](rng_states, 10000, out)\n",
    "print('pi:', out.mean())\n",
    "print(time.time()-start)\n",
    "import time\n",
    "start = time.time()\n",
    "threads_per_block = 64\n",
    "blocks = 24\n",
    "rng_states = create_xoroshiro128p_states(threads_per_block * blocks, seed=1)\n",
    "out = np.zeros(threads_per_block * blocks, dtype=np.float32)\n",
    "compute_pi[blocks, threads_per_block](rng_states, 10000, out)\n",
    "print('pi:', out.mean())\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多维度随机数生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50002056\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "from numba.cuda.random import (create_xoroshiro128p_states,\n",
    "xoroshiro128p_uniform_float32)\n",
    "import numpy as np\n",
    "\n",
    "@cuda.jit\n",
    "def random_3d(arr, rng_states):\n",
    "    # Per-dimension thread indices and strides\n",
    "    startx, starty, startz = cuda.grid(3)\n",
    "    stridex, stridey, stridez = cuda.gridsize(3)\n",
    "\n",
    "    # Linearized thread index\n",
    "    tid = (startz * stridey * stridex) + (starty * stridex) + startx\n",
    "\n",
    "    # Use strided loops over the array to assign a random value to each entry\n",
    "    for i in range(startz, arr.shape[0], stridez):\n",
    "        for j in range(starty, arr.shape[1], stridey):\n",
    "            for k in range(startx, arr.shape[2], stridex):\n",
    "                arr[i, j, k] = xoroshiro128p_uniform_float32(rng_states, tid)\n",
    "\n",
    "# Array dimensions\n",
    "X, Y, Z = 70, 900, 719\n",
    "\n",
    "# Block and grid dimensions\n",
    "bx, by, bz = 8, 8, 8\n",
    "gx, gy, gz = 16, 16, 16\n",
    "\n",
    "# Total number of threads\n",
    "nthreads = bx * by * bz * gx * gy * gz\n",
    "\n",
    "# Initialize a state for each thread\n",
    "rng_states = create_xoroshiro128p_states(nthreads, seed=1)\n",
    "\n",
    "# Generate random numbers\n",
    "arr = cuda.device_array((X, Y, Z), dtype=np.float32)\n",
    "random_3d[(gx, gy, gz), (bx, by, bz)](arr, rng_states)\n",
    "arr = arr.copy_to_host()\n",
    "print(arr.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设备操作\n",
    "***设备选择***   \n",
    "    numba.cuda.select_device(device_id):选择设备\n",
    "    numba.cuda.close()：关闭设备  \n",
    "***查看设备列表***    \n",
    "    numba.cuda.gpus\n",
    "    numba.cuda.cudadrv.devices.gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 案例1：矩阵向乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, float32\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pointrcnn]",
   "language": "python",
   "name": "pointrcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
