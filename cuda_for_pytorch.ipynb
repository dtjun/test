{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ****用cuda给pytorch写算子****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***1. cuda文件***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****头文件：****  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <ATen/ATen.h> //主要是用到了一个类型at::Tensor\n",
    "\n",
    "#include <cuda.h> //编写cuda代码必备\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#include <vector> //可能用到了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 首先是写要在cpp文件中调用的函数：\n",
    "    写法就是C++的写法，主要就是调用核函数\n",
    "    （1）设置线程块数和线程数，初始化返回值\n",
    "    （2）调用核函数：\n",
    "        AT_DISPATCH_FLOATING_TYPES(); 案例如下\n",
    "        参数依次代表：输入数据类型,\n",
    "                    操作标识符（自定义）,\n",
    "                    ([&]{核函数;})\n",
    "        AT_DISPATCH_FLOATING_TYPES(输入数据类型,\n",
    "                                    操作标识符（自定义）,\n",
    "                                    ([&]{核函数;}));\n",
    "    （3）可以有返回值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AT_DISPATCH_FLOATING_TYPES(input.type(), \"sigmoid_forward_cuda\", ([&] {sigmoid_cuda_forward_kernel<scalar_t><<<blocks, threads>>>(input.data<scalar_t>(),output.data<scalar_t>());}));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ****核函数核设备函数****\n",
    "    待学"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ****2. cpp文件****\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **头文件：**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <torch/torch.h>  \n",
    "#include \\<vector>  \n",
    "#好像不需要引入cuda文件，直接调用函数即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ****声明在cuda中写好的函数****   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at::Tensor sigmoid_cuda_forward(at::Tensor input);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ****写一些常用的检测输入输出类型的函数****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
    "#define CHECK_CONTIGUOUS(x) AT_ASSERTM(x.is_contiguous(), #x \" must be contiguous\")\n",
    "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ****把检测类型的函数和声明的函数再次封装****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at::Tensor sigmoid_forward(at::Tensor input) {\n",
    "    CHECK_INPUT(input);\n",
    "    return sigmoid_cuda_forward(input);}\n",
    "\n",
    "at::Tensor sigmoid_backward(at::Tensor grad_output,at::Tensor output) {\n",
    "    CHECK_INPUT(grad_output);\n",
    "    CHECK_INPUT(output);\n",
    "    return sigmoid_cuda_backward(grad_output,output);}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ****给python定义和C++之间定义一个接口****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {  \n",
    "  m.def(\"方法名\", &再次封装好的函数名, \"描述信息\");  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
    "  m.def(\"forward\", &sigmoid_forward, \"sigmoid forward (CUDA)\");\n",
    "  m.def(\"backward\", &sigmoid_backward, \"sigmoid backward (CUDA)\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ****3. setup.py文件****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****导入包****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setuptools import setup\n",
    "from torch.utils.cpp_extension import BuildExtension, CUDAExtension, CppExtension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****setup()****  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup(\n",
    "    name='sigmoid_cuda_linear_cpp',  #好像随便起\n",
    "    ext_modules=[                    #一个列表\n",
    "        CUDAExtension('sigmoid_cuda', [ #CUDAExtension('包名',['cpp文件路径','cuda文件路径'])，里面可以有多个cpp和cuda文件\n",
    "            'sigmoid_cuda.cpp',\n",
    "            'sigmoid_cuda_kernel.cu',\n",
    "        ]),\n",
    "        CppExtension('linear_cpp', ['linear.cpp']) #CppExtension('包名', ['cpp文件路径'])\n",
    "    ],\n",
    "    cmdclass={\n",
    "        'build_ext': BuildExtension  #通用\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ****4. 在python文件中调用****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****继承Function类，重写forward和backward方法****  \n",
    "    写每个方法都需要@staticmethod进行修饰\n",
    "    方程写好之后都需要.apply转化为可以调用的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Parameter\n",
    "from torch.autograd import Function\n",
    "\n",
    "import torch\n",
    "import linear_cpp\n",
    "import sigmoid_cuda\n",
    "\n",
    "class DenseFunction(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        output = linear_cpp.forward(input, weight, bias)\n",
    "        output = sigmoid_cuda.forward(output)\n",
    "        ctx.save_for_backward(input, weight, bias, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output): \n",
    "        input, weight, bias, output = ctx.saved_variables\n",
    "        grad_sigmoid = sigmoid_cuda.backward(grad_output, output)\n",
    "        grad_output = grad_sigmoid * grad_output\n",
    "        grad_input, grad_weight, grad_bias = linear_cpp.backward(grad_output, input, weight, bias)\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "        \n",
    "class Dense(Module):\n",
    "\n",
    "    def __init__(self, input_features, output_features, bias=True):\n",
    "        super(Dense, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        self.weight = Parameter(torch.Tensor(output_features, input_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(output_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.weight.data.uniform_(-0.1, 0.1)\n",
    "        if bias is not None:\n",
    "            self.bias.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return DenseFunction.apply(input, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatherOperation(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, features: torch.Tensor, idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param ctx:\n",
    "        :param features: (B, C, N)\n",
    "        :param idx: (B, npoint) index tensor of the features to gather\n",
    "        :return:\n",
    "            output: (B, C, npoint)\n",
    "        \"\"\"\n",
    "        assert features.is_contiguous()\n",
    "        assert idx.is_contiguous()\n",
    "\n",
    "        B, npoint = idx.size()\n",
    "        _, C, N = features.size()\n",
    "        output = torch.cuda.FloatTensor(B, C, npoint)\n",
    "\n",
    "        pointnet2.gather_points_wrapper(B, C, N, npoint, features, idx, output)\n",
    "\n",
    "        ctx.for_backwards = (idx, C, N)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        idx, C, N = ctx.for_backwards\n",
    "        B, npoint = idx.size()\n",
    "\n",
    "        grad_features = Variable(torch.cuda.FloatTensor(B, C, N).zero_())\n",
    "        grad_out_data = grad_out.data.contiguous()\n",
    "        pointnet2.gather_points_grad_wrapper(B, C, N, npoint, grad_out_data, idx, grad_features.data)\n",
    "        return grad_features, None\n",
    "\n",
    "\n",
    "gather_operation = GatherOperation.apply"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pointrcnn]",
   "language": "python",
   "name": "pointrcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
